<!doctype html><html lang=en><head><link rel=stylesheet href=../../../../css/styles.css type=text/css><link rel=apple-touch-icon sizes=180x180 href=../../../../apple-touch-icon.png><link rel=icon type=image/svg+xml sizes=16x16 href=../../../../favicon-16x16.svg><link rel=icon type=image/svg+xml sizes=32x32 href=../../../../favicon-32x32.svg><link rel=icon type=image/svg+xml sizes=120x120 href=../../../../favicon-120x120.svg><link rel=manifest href=../../../../site.webmanifest><link rel=mask-icon href=../../../../safari-pinned-tab.svg color=#5a2673><link rel=author href=https://rdner.de/about type=text/html><link rel=license href=http://creativecommons.org/licenses/by-nc-sa/4.0/ type=text/html><title>rdner's blog: Denis Rechkunov – My automated backups to AWS S3 with client-side AES256</title>
<meta charset=utf-8><meta name=apple-mobile-web-app-title content="rdner's blog"><meta name=application-name content="rdner's blog"><meta name=msapplication-TileColor content="#5a2673"><meta name=theme-color content="#5a2673"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="Denis Rechkunov"><meta name=description content="I'm very paranoid when it comes to my data. For me, the only backup solution that I could possibly use, must be simple and must have client-side AES256 encryption with the passphrase that only I would know."><meta name=keywords content="security,encryption,backup,s3,aws,bash,infosec"><meta name=twitter:card content="summary"><meta name=twitter:title content="rdner's blog: Denis Rechkunov – My automated backups to AWS S3 with client-side AES256"><meta name=twitter:description content="I'm very paranoid when it comes to my data. For me, the only backup solution that I could possibly use, must be simple and must have client-side AES256 encryption with the passphrase that only I would know."><meta name=twitter:image content="https://rdner.de/posts/tech/security/s3-aes256-backups/cover.jpg"><meta property="og:type" content="website"><meta property="og:url" content="https://rdner.de/posts/tech/security/s3-aes256-backups/"><meta property="og:title" content="rdner's blog: Denis Rechkunov – My automated backups to AWS S3 with client-side AES256"><meta property="og:description" content="I'm very paranoid when it comes to my data. For me, the only backup solution that I could possibly use, must be simple and must have client-side AES256 encryption with the passphrase that only I would know."><meta property="og:image" content="https://rdner.de/posts/tech/security/s3-aes256-backups/cover.jpg"><link rel=next href=../../../../posts/motorcycle/granada-spain/ type=text/html><link rel=prev href=../../../../posts/tech/security/security-tips/ type=text/html></head><body><section class=site-header><a class=logo href=https://rdner.de/><img alt="website logo" src=../../../../images/logo.svg></a><nav><a class=nav-item href=../../../../posts/ title="My blog posts">Posts</a>
<a class=nav-item href=../../../../talks/ title="My talks">Talks</a>
<a class=nav-item href=../../../../cv.html title="My most recent CV">CV</a>
<a class=nav-item href=https://github.com/rdner rel="nofollow noreferrer" target=_blank><img alt="github account" class=icon src=../../../../images/github.svg>
</a><a class=nav-item href=../../../../index.xml target=_blank><img alt="RSS feed" class=icon src=../../../../images/rss.svg></a></nav></section><main><article><header class=article-header style=background-image:url(/posts/tech/security/s3-aes256-backups/cover.jpg)><h1>My automated backups to AWS S3 with client-side AES256</h1><div class=article-header-bottom><div class=article-summary-card><a class=article-author-link href=../../../../about><img alt="author picture" class=article-author-pic src="https://www.gravatar.com/avatar/0f18ade06d929797f7c7e418c509a0b1?s=512"></a><div class=article-facts-container><div class=article-author>Author:&nbsp;<a href=../../../../about>Denis Rechkunov</a></div><div class=article-facts><span>Published&nbsp;on&nbsp;<time datetime="2019-12-08 00:00:00 +0000 UTC">Dec 8, 2019</time></span>
<span>~1400&nbsp;words</span>
<span>7&nbsp;min&nbsp;read</span></div></div><p>I'm very paranoid when it comes to my data. For me, the only backup solution that I could possibly use, must be simple and must have client-side AES256 encryption with the passphrase that only I would know.</p></div><div class=article-header-credit>Image credit: <a href=The%20bank%20vault%20in%20NoMad%20Downtown%20LA.%20Image%20by%20Benoit%20Linero. rel="nofollow noreferrer" target=_blank>The bank vault in NoMad Downtown LA. Image by Benoit Linero.</a></div></div></header><section class=content-container><section class=content><div class=breadcrumbs><span class=breadcrumb><a href=../../../../>Hello, World!</a></span>
<span class=breadcrumb><a href=../../../../posts/>Blog posts</a></span>
<span class=breadcrumb><a href=../../../../posts/tech/>Technology</a></span>
<span class=breadcrumb><a href=../../../../posts/tech/security/>Information Security</a></span>
<span class=breadcrumb><a href=../../../../posts/tech/security/s3-aes256-backups/>My automated backups to AWS S3 with client-side AES256</a></span></div><aside class=content-toc><nav id=TableOfContents><ul><li><a href=#what-i-was-looking-for-requirements>What I was looking for. Requirements</a></li><li><a href=#3-2-1-backup-rule>3-2-1 Backup Rule</a></li><li><a href=#first-step-encryption>First Step: Encryption</a></li><li><a href=#second-step-storage-in-s3>Second Step: Storage in S3</a><ul><li><a href=#create-a-user>Create a user</a></li><li><a href=#configure-your-aws-cli>Configure your AWS CLI</a></li><li><a href=#create-s3-bucket>Create S3 bucket</a></li><li><a href=#create-a-policy>Create a policy</a></li><li><a href=#use-the-backup-bash-script>Use the backup bash script</a></li></ul></li><li><a href=#fulfilled-requirements>Fulfilled Requirements</a></li></ul></nav></aside><h1 id=what-i-was-looking-for-requirements>What I was looking for. Requirements</h1><p>The reason I didn&rsquo;t just start using some backup software with built-in encryption is a pure lack of trust. So, if you&rsquo;re fine with someone managing your backups, you might think the approach described here is a total overkill. And it&rsquo;s okay. But there are some people out there who have the same trust issues that I do and they might find this article useful, so I write it for them. And also for myself, as a write-up what I did.</p><p>What are my personal requirements to a backup automation:</p><ul><li>The storage back-end must be highly available</li><li>The storage back-end must have versioning</li><li>The client must <strong>not</strong> have permission to destroy or corrupt data</li><li>The client must be transparent and obvious in how it works</li><li>The client must be smart enough to synchronize backups — uploads changes only</li><li>The encryption of my backups must happen on my machine and should prompt for a passphrase during the process. It should be technically impossible for someone except me to access the backups.</li></ul><p>I know, one can argue about the last one because this passphrase is a shared secret created by a human but I can assure you, nobody else knows my passphrase (unless somebody installs a key-logger software on my computer) and the passphrase is really strong (long, special characters, different casing, numbers, etc).</p><h1 id=3-2-1-backup-rule>3-2-1 Backup Rule</h1><p>There is a common rule of backup that says you must be:</p><ul><li>Making at least 3 copies of data, located on physically different storage media;</li><li>Keeping these copies in no less than 2 different formats;</li><li>Always storing at least 1 of these backups off-site (e.g. on the commercial cloud account).</li></ul><p>I don&rsquo;t know where exactly this rule came from but you can definitely find some mentions on the internet (e.g <a href=https://www.backblaze.com/blog/the-3-2-1-backup-strategy/>here</a> and <a href=https://www.nakivo.com/blog/3-2-1-backup-rule-efficient-data-protection-strategy/>here</a>).</p><p>This rule makes sense to me, so I would like to follow this rule as much as I can.</p><p>So, for me it would be:</p><ul><li>3 copies of data: 1 on AWS, 1 on a mSD card, 1 on a USB thumb drive</li><li>2 formats: I guess this is an exception for me, I store only AES256 encrypted TAR files</li><li>1 off-site: 1 backup is on AWS, so technically it&rsquo;s off-site of my laptop. Or I can say that my thumb drive or mSD card is off-site, whatever.</li></ul><p>Before we talk about how I actually store backups, let&rsquo;s talk about the encryption I use.</p><h1 id=first-step-encryption>First Step: Encryption</h1><p>Some time ago I wrote a bash script for encrypting a given directory. It&rsquo;s very simple and is based on the GPG symmetric mode and AES256 algorithm. This script creates a TAR archive and then directs it to GPG that prompts for a passphrase and performs the encryption.</p><p>You can find the source code <a href=https://github.com/rdner/dotfiles/blob/master/files/scripts/vault>here on GitHub</a>.</p><p>This script produces <code>*.enc</code> files that I can upload or store anywhere without fear that somebody can get access to the content.</p><p>For me, it&rsquo;s the simplest and most reliable solution: GPG is a very reliable tool and I understand 100% what my script is doing because it&rsquo;s so simple.</p><h1 id=second-step-storage-in-s3>Second Step: Storage in S3</h1><p>We&rsquo;re not going to talk about my physical devices, it&rsquo;s pretty much manual work there.
What I want to talk about is how I use AWS CLI to sync the local directory where I store backups with a bucket folder in S3.</p><p>Obviously, you need an AWS account for everything that follows.</p><p>To prepare everything we need:</p><ul><li>Create a user in IAM for running AWS CLI</li><li>Configure your AWS CLI</li><li>Create an S3 bucket and a folder for backups</li><li>Create a policy for the user, so it has write-only access to the bucket folder</li></ul><p>And after the preparation we can just use another bash script.</p><h2 id=create-a-user>Create a user</h2><ul><li>Go to the <a href=https://console.aws.amazon.com/iam/home#/users>IAM console</a> and click <code>Add user</code>.</li><li>Type in a desirable user name and check <strong>only</strong> <code>Programmatic access</code>. Click <code>Next: Permissions</code></li><li><strong>Don&rsquo;t</strong> add the user to any groups, just skip the step clicking on <code>Next: Tags</code></li><li>No tags, required, just skip clicking <code>Next: Review</code></li><li>Review the user and submit with <code>Create user</code>. It&rsquo;s okay to have a warning <code>This user has no permissions</code>, we will address it later.</li></ul><h2 id=configure-your-aws-cli>Configure your AWS CLI</h2><ul><li>Install AWS CLI, follow <a href=https://docs.aws.amazon.com/cli/latest/userguide/install-cliv1.html>the official instructions</a>. e.g. in Debian it&rsquo;s just <code>sudo apt install awscli</code>.</li><li>Go to <a href=https://console.aws.amazon.com/iam/home#/users>IAM console</a></li><li>Click on your user you created</li><li>Go to <code>Security credentials</code> tab</li><li>Click <code>Create access key</code> and you&rsquo;ll a dialog with <code>Access key ID</code> and <code>Secret access key</code>. <strong>Don&rsquo;t close this dialog</strong>*.</li><li>Go to your terminal and type <code>awscli configure</code></li><li>Follow the <a href=https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html>official instructions</a>, you&rsquo;ll need the <code>Access key ID</code> and <code>Secret access key</code> from the previous step</li></ul><p>Now you should have a ready-to-work AWS CLI setup on your computer.</p><h2 id=create-s3-bucket>Create S3 bucket</h2><ul><li>Go to the <a href=https://s3.console.aws.amazon.com/s3/home>S3 console</a> and click <code>Create bucket</code></li><li>Type in a desirable name, select a region and click <code>Next</code></li><li>On the next <code>Configure options</code> step check <code>Keep all versions of an object in the same bucket</code> and click <code>Next</code>.</li><li>On the next <code>Set permissions</code> step make sure it says <code>Block all public access</code> and click <code>Next</code>.</li><li>After reviewing and creating the bucket browse the bucket and create a folder for backups in it</li></ul><h2 id=create-a-policy>Create a policy</h2><p>Now it&rsquo;s time to allow our CLI user to upload new backups, for this we need to create a policy:</p><ul><li>Go to the <a href=https://console.aws.amazon.com/iam/home#/users>IAM console</a> and click on the user you created earlier</li><li>Click <code>Add inline policy</code></li><li>AWS has this visual editor by default but it&rsquo;s easier to put this JSON example here instead of describing where to click:</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;Version&#34;</span>: <span style=color:#e6db74>&#34;2012-10-17&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;Statement&#34;</span>: [
</span></span><span style=display:flex><span>        {
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;Sid&#34;</span>: <span style=color:#e6db74>&#34;ListBackups&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;Effect&#34;</span>: <span style=color:#e6db74>&#34;Allow&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;Action&#34;</span>: <span style=color:#e6db74>&#34;s3:ListBucket&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;Resource&#34;</span>: <span style=color:#e6db74>&#34;arn:aws:s3:::&lt;bucket-name&gt;&#34;</span>
</span></span><span style=display:flex><span>        },
</span></span><span style=display:flex><span>        {
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;Sid&#34;</span>: <span style=color:#e6db74>&#34;UploadBackups&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;Effect&#34;</span>: <span style=color:#e6db74>&#34;Allow&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;Action&#34;</span>: <span style=color:#e6db74>&#34;s3:PutObject&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#f92672>&#34;Resource&#34;</span>: <span style=color:#e6db74>&#34;arn:aws:s3:::&lt;bucket-name&gt;/&lt;backup-folder&gt;/*&#34;</span>
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>    ]
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>Replace <code>&lt;bucket-name></code> and <code>&lt;backup-folder></code> with your values. <code>s3:ListBucket</code> permission is required for the <code>aws s3 sync</code> command that we&rsquo;re going to use, so it&rsquo;s able to compare file sizes and modification dates.</p><h2 id=use-the-backup-bash-script>Use the backup bash script</h2><p>I wrote a simple bash script that finds all files with <code>.enc</code> extension in the current directory and its sub-directories and uploads them to the given location on S3.</p><p>You can view and copy the script from my <a href=https://github.com/rdner/dotfiles/blob/master/files/scripts/backup>repository on GitHub</a>.</p><p>Now, when our S3 bucket exists and we have an IAM user for our CLI that has a policy to write to a backup folder on the bucket, we can finally run the script:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-bash data-lang=bash><span style=display:flex><span>backup s3://&lt;bucket-name&gt;/&lt;backup-folder&gt;
</span></span></code></pre></div><p>This script is using <code>aws s3 sync</code> command that checks if the local file matches a remote file by size and modification date and if they match, it does not upload your local file anywhere.</p><h1 id=fulfilled-requirements>Fulfilled Requirements</h1><p>As you might remember, I had my very strict requirements in the beginning, how does this approach fulfill them? Let&rsquo;s have a look:</p><ul><li>The storage back-end must be highly available — it&rsquo;s S3</li><li>The storage back-end must have versioning — we turned it on when created the bucket</li><li>The client must <strong>not</strong> have permission to destroy or corrupt data — our policy makes sure the script is allowed to only put objects, can&rsquo;t read or delete them. The user is allowed to upload a file with the same name but it won&rsquo;t corrupt data just will create a new version of it</li><li>The client must be transparent and obvious in how it works — just small bash scripts that you can read and understand</li><li>The client must be smart enough to synchronize backups — <code>aws s3 sync</code> uploads only changed or new files to the bucket</li><li>The encryption of my backups must happen on my machine — the first short script also fulfills this requirement for me.</li></ul><p>I admit, this setup is not for everyone and it&rsquo;s far from being simple but if you&rsquo;re working with AWS already it might suit you.</p><p>I hope you learned something from this article. Take care.</p><p>P.S. Please plan your expenses using S3 on AWS in advance. I&rsquo;m not responsible for any unexpected costs.</p><hr><div class=tags>Tags:
<a href=../../../../tags/privacy>privacy</a>
<a href=../../../../tags/encryption>encryption</a></div></section><section class="content content-list"><h4 class=centered>Other posts</h4><ul class=article-feed><li><a href=../../../../posts/sport/im-70.3/><article class="article-item with-cover" style=background-image:url(/posts/sport/im-70.3/cover.jpg)><header><h3>My road to IRONMAN 70.3</h3></header><div class=article-item-details><time datetime="2023-10-01 00:00:00 +0000 UTC">Oct 1, 2023</time><p>This is a story of a big life change.</p></div></article></a></li><li><a href=../../../../posts/sport/why-run/><article class="article-item with-cover" style=background-image:url(/posts/sport/why-run/cover.jpg)><header><h3>Why run?</h3></header><div class=article-item-details><time datetime="2022-11-11 00:00:00 +0000 UTC">Nov 11, 2022</time><p>Recently quite a few people asked me "Why are you running?", sometimes even followed by something like "it's stupid" or whatever to hint me that I should not. I hope to be able to just send a link to this article next time.</p></div></article></a></li><li><a href=../../../../posts/tech/security/privacy-tips/><article class="article-item with-cover" style=background-image:url(/posts/tech/security/privacy-tips/cover.jpg)><header><h3>Paranoid Habits. Privacy Tips</h3></header><div class=article-item-details><time datetime="2021-03-10 12:01:16 +0100 +0100">Mar 10, 2021</time><p>This is another write up of my recent talk where I share some information about the modern era of data collection and how you can keep your privacy at least partially.</p></div></article></a></li></ul></section></section></article></main><footer><span xmlns:dct=http://purl.org/dc/terms/ href=http://purl.org/dc/dcmitype/Text property="dct:title" rel=dct:type>rdner's blog
</span>by <a xmlns:cc=http://creativecommons.org/ns# href=https://rdner.de/about property="cc:attributionName" rel=cc:attributionURL>Denis Rechkunov</a>&nbsp;(<a href=mailto:denis@rdner.de>denis@rdner.de</a>) is licensed under a <a rel=license href=http://creativecommons.org/licenses/by-nc-sa/4.0/>Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a><a rel=license href=http://creativecommons.org/licenses/by-nc-sa/4.0/>&nbsp;<img alt="Creative Commons License" style=border-width:0 src=../../../../images/cc-by-nc-sa.png></a></footer></body></html>